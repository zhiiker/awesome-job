时间：2019年4月8日 ~ 2019年4月14日

学习人|学习任务|学习心得
------ | ------ | ------ 
李小川  | 正则化逻辑回归|对线性可分的模型进行了温故后又复习了一遍线性不可分的算法，才算缕清楚了前后递进的关系，以及中间几个步骤的意义，如特征映射，正则化。同时对numpy的几个函数做了详细了解如numpy.exp(),numpy.log()。 具体的总结在博客https://blog.csdn.net/lixc316/article/details/89070140 
刘天宇  | 金融数据分析　| 通过这周的学习，基本了解如何使用机器学习来做金融数据分析，学会使用LSTM神经网络预测股票价格，学习了如何通过机器学习预测股票涨跌。
袁林     | 机器学习前三周总结| 这周总结了下前三周常用的几个函数用法，zip() 函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。numpy.linspace（）函数，np.meshgrid函数接受两个一维数组，并根据两个数组的所有（a,b）对生成一个二维矩阵。reshape()方法来更改数组的形状，还有一些科学算法的传参（内置算法还不太明白，需要弄明白）。
陆开胜 | 聚类算法 | 这周大部分时间在翻译小论文，在学习yolo算法的过程了解到其中几种anchor prior的尺寸是通过k-means算法聚类得到的，所以对聚类算法进行学习了一遍。 一些总结https://blog.csdn.net/shaile7593/article/details/89300177
王鸿博 | keras框架学习 | 因为复现论文需要，这周学习了一下keras框架，keras是一个基于python的深度学习库，以tensorflow或者theano作为backend，封装了许多常用的函数，非常简单就能建立起一个神经网络。
成文浩 |Few-Shot Learning with Graph Neural Networks| 见附页 
潘瑞 | 西瓜书第三章 |  本周学习机器学习第三章节。学习过程复习了两个基础知识，最小二乘法和极大似然法。几个要点包括：1）线性回归。使用均方误差和最小二乘法来进行计算的；2）对数几率函数。对几率（正例的可能性和反例可能性的比值）取对数就得到“对数几率“。3）线性判别分析。二分类和多分类问题。4）二分类三个步骤，给定样例；找到一条满足“同类近、异类远”的投影直线；分类依靠投影后点的位置来确定；5）多分类。基本思路是“拆解法”。OvO，OvR，MvM。
|王汇源|统计学习第二章|感知机，概念：二类分类的线性分类模型，输入是特征向量，输出是类别，取+1和-1。目的：求出将训练数据线性划分的分离超平面。自己简单理解的思路：对每个实例对应于n维空间的一个点，并且是可分类的。那么一定存在一个超平面使得这两类实例，各位于超平面的一侧。显然这个超平面不止一个，但是为了找到一个最适合的超平面，我们要使每一个实例到超平面的距离求和代表误差，当这个求和最小时，超平面最好。二分类模型：f(x)=sign(w∗x+b)，损失函数L(w,b)=−Σyi(w∗xi+b),代码复现了黄博的代码，感觉和大家差距好大，学的也很慢，慢慢加油学！！

# 附页 成文浩
## 输入

$\mathcal{T}=\left\{\left\{\left(x_{1}, l_{1}\right), \ldots\left(x_{s}, l_{s}\right)\right\},\left\{\tilde{x}_{1}, \ldots, \tilde{x}_{r}\right\},\left\{\overline{x}_{1}, \ldots, \overline{x}_{t}\right\} ; l_{i} \in\{1, K\}, x_{i}, \tilde{x}_{j}, \overline{x}_{j} \sim \mathcal{P}_{l}\left(\mathbb{R}^{N}\right)\right\}$

其中s为有标签的样本数量，r为无标签的样本数量，t为待分类的样本数量，其中所有x独立同分布。

## 第一步

将输入 $\mathcal{T} $ 转换为全连接图$G_{\mathcal{T}}=(V, E)$ ，其中$v_{a} \in V$  代表图片$x$ (包括有标签和无标签的)。

## 初始化

图$G$ 的初始值通过下式得到$\mathbf{x}_{i}^{(0)}=\left(\phi\left(x_{i}\right), h\left(l_{i}\right)\right)$
其中$\phi\left(x_{i}\right)$ 是一个CNN，$h(l_{i})$ 是独热码。
