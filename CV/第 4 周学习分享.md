时间：2019年4月15日 ~ 2019年4月21日

学习人|学习任务|学习心得和参考资料
------ | ------ | ------ 
安军刚|学习视频目标跟踪|多目标跟踪是目前在军事及社会安全非常关注的焦点问题，因此我们主要是对2019年CVPR的oral论文SiamRPN++学习总结如下：动机：基于SiamFC跟踪需要网络满足平移不变性，但是siamfc中的padding会破坏该性质，领域和联通性应该能保证对称性，这里个人认为前后景通过NMS目前仍然是一个学术痛点。为了适应视频跟踪我们需要对基础模型的stride从32改为8，这样能保证定位准确性。<br>改进：scross correlation：该模块主要是通过搜索区域上滑窗获取不同的位置信息，up-channel cross correlation是在cross correlation前面加两个卷积层，通道数增加再通过卷积输出保持一致。
安静 | EAST论文总结 | 优点：1.估计到文字区域大小各异的问题 2. U-shape 思想，逐渐合并特征图计算量小 网络框架：特征提取主干，特征整合和输出层 
唐洋|深度学习中的Normalization模型| 主要学习了BN、Layer Normalization、Instance Normalization及Group Normalization,具体如下：
帅张|CV中交叉熵和聚类算法|K-means Algorithm：1、随机选取K个点，即聚类中心；2、计算每个数据点到聚类中心的距离，将距离最小的聚类中心与之关联；3、把关联到同一聚类中心的数据点分为一组；4、将每组数据点的中心（算均值）记为新的聚类中心；5、重复1-4步，直至聚类中心不再变化。交叉熵：
> 注：大家打卡时，为了方便自己和拯救他人，请注意**格式美观**，每段用心编辑的文字，都代表了我们的学习态度。如果表格中无法很好的显示格式，可以在文档后附上打卡内容，如下

### 安静的学习心得
#### EAST流程<br>
* Multi-channel FCN<br>
  *  相比于R-CNN/Fast R-CNN等提取的Feature map一步到位，EAST提取的Feasture map经过4次分支的整合得到。<br>
    * 5个卷积阶段（卷积层/池化层）输出4个不同大小的特征图。
    * 特征图合并过程中，每一个阶段的特征图使用上采样层（unpooling layer）使其大小翻倍，经过1乘1卷积层减少通道数，3乘3卷积层融合信息，最后一次的融合结果交给输出层。<br>
    * 输出层将收到的特征图分别映射为1通道的score feature和多通道的geometry map(RBOX/QUAD)。RBOX，由4个通道的轴向包围盒（AABB）R和1个通道的旋转角度θ表示。其中4个通道分别表示定位点到上右下左边界的距离；对于QUAD，使用8个通道表示四边形四个顶点{δxi, δyi | i<-{1,2,3,4}}到像素位置的坐标变换。<br>
* Thresholding & NMS<br>
  * 使用NMS过滤冗余的候选框，缩小时间复杂度。来自相邻像素的几何形状往往和高度相关，作者提出合并同一行的几何形状，合并的四边形的坐标是给定的两个四边形的加权平均。<br>


### 唐洋的学习心得（深度学习中的Normalization模型）

1. Normalization到底是在做什么？

	规范化就是希望对x进行一定的变换（规范化函数），使得规范化后的数值满足一定的特性。

	至于深度学习中的Normalization，因为神经网络里主要有两类实体：神经元或者连接神经元的边，所以按照规范化操作涉及对象的不同可以分为两大类，一类是对第L层每个神经元的激活值或者说对于第L+1层网络神经元的输入值进行Normalization操作，比如BatchNorm/LayerNorm/InstanceNorm/GroupNorm等方法都属于这一类；另外一类是对神经网络中连接相邻隐层神经元之间的边上的权重进行规范化操作，比如Weight Norm就属于这一类。广义上讲，一般机器学习里看到的损失函数里面加入的对参数的的L1/L2等正则项，本质上也属于这第二类规范化操作。L1正则的规范化目标是造成参数的稀疏化，就是争取达到让大量参数值取得0值的效果，而L2正则的规范化目标是有效减小原始参数值的大小。有了这些规范目标，通过具体的规范化手段来改变参数值，以达到避免模型过拟合的目的。

2. Normalization加入的位置？

	第一种是原始BN论文提出的，放在激活函数之前；另外一种是后续研究提出的，放在激活函数之后，不少研究表明将BN放在激活函数之后效果更好。

	### 公式为：**a\_i<sup>norm</sup>=γ\_i * (a\_i - μ)/δ\_i + β_i** 
	其中,μ和δ分别为神经元集合S的均值和标准差。

	### 不同Normalization方法确定S集合的方式不同。

3. Batch Normalization 如何做？

	3.1. 前向神经网络中的BN
	
	对于前向神经网络来说，BatchNorm在计算隐层某个神经元k激活的规范值的时候，对应的神经元集合S范围是如何划定呢？因为对于Mini-Batch训练方法来说，根据Loss更新梯度使用Batch中所有实例来做，所以对于神经元k来说，假设某个Batch包含n个训练实例，那么每个训练实例在神经元k都会产生一个激活值，也就是说Batch中n个训练实例分别通过同一个神经元k的时候产生了n个激活值，BatchNorm的集合S选择入围的神经元就是这n个同一个神经元被Batch不同训练实例激发的激活值。划定集合S的范围后，采用上述公式即可完成规范化操作。
	
	3.2 CNN网络中的BN
	
	那么在卷积层中，如果要对通道激活二维平面中某个激活值进行Normalization操作，怎么确定集合S的范围呢？类似于前向神经网络中的BatchNorm计算过程，对于Mini-Batch训练方法来说，反向传播更新梯度使用Batch中所有实例的梯度方向来进行，所以对于CNN某个卷积层对应的输出通道k来说，假设某个Batch包含n个训练实例，那么每个训练实例在这个通道k都会产生一个二维激活平面，也就是说Batch中n个训练实例分别通过同一个卷积核的输出通道k的时候产生了n个激活平面。假设激活平面长为5，宽为4，则激活平面包含20个激活值，n个不同实例的激活平面共包含20\*n个激活值。那么BatchNorm的集合S的范围就是由这20*n个同一个通道被Batch不同训练实例激发的激活平面中包含的所有激活值构成。划定集合S的范围后，激活平面中任意一个激活值都需进行Normalization操作，采用上述公式即可完成规范化操作。这样即完成CNN卷积层的BatchNorm转换过程。
	
	从理论上讲，类似的BatchNorm操作也可以应用在RNN上，事实上也有不少研究做了尝试，但是各种实验证明其实这么做没什么用。
	
	3.3 Batch Norm的缺点
	- 如果Batch Size太小，则BN效果明显下降。
	- 对于有些像素级图片生成任务来说，BN效果不佳。
	- RNN等动态网络使用BN效果不佳且使用起来不方便。
	- 训练时和推理时统计量不一致
	
	上面所列BN的四个缺点，表面看是四个问题，其实深入思考，都指向了幕后同一个黑手，这个隐藏在暗处的黑手是谁呢？就是BN要求计算统计量的时候必须在同一个Mini-Batch内的实例之间进行统计，因此形成了Batch内实例之间的相互依赖和影响的关系。如何从根本上解决这些问题？一个自然的想法是：把对Batch的依赖去掉，转换统计集合范围。在统计均值方差的时候，不依赖Batch内数据，只用当前处理的单个训练数据来获得均值方差的统计量，这样因为不再依赖Batch内其它训练数据，那么就不存在因为Batch约束导致的问题。在BN后的几乎所有改进模型都是在这个指导思想下进行的。

4. Layer Normalization 如何做？
	
	为了能够在只有当前一个训练实例的情形下，也能找到一个合理的统计范围，一个最直接的想法是：前向神经网络(如MLP)的同一隐层自己包含了若干神经元；同理，CNN中同一个卷积层包含k个输出通道，每个通道包含m\*n个神经元，整个通道包含了k\*m*n个神经元；类似的，RNN的每个时间步的隐层也包含了若干神经元。那么我们完全可以直接用同层隐层神经元的响应值作为集合S的范围来求均值和方差。这就是Layer Normalization的基本思想。

	BN在RNN中用起来很不方便，而Layer Normalization这种在同隐层内计算统计量的模式就比较符合RNN这种动态网络，目前在RNN中貌似也只有LayerNorm相对有效，但Layer Normalization目前看好像也只适合应用在RNN场景下，在CNN等环境下效果是不如BatchNorm或者GroupNorm等模型的。
5. Instance Normalization 如何做？
	
	Layer Normalization在抛开对Mini-Batch的依赖目标下，为了能够统计均值方差，很自然地把同层内所有神经元的响应值作为统计范围，那么我们能否进一步将统计范围缩小？对于CNN明显是可以的，因为同一个卷积层内每个卷积核会产生一个输出通道，而每个输出通道是一个二维平面，也包含多个激活神经元，自然可以进一步把统计范围缩小到单个卷积核对应的输出通道内部。对于图中某个卷积层来说，每个输出通道内的神经元会作为集合S来统计均值方差。
	
	对于RNN或者MLP，如果在同一个隐层类似CNN这样缩小范围，那么就只剩下单独一个神经元，输出也是单值而非CNN的二维平面，这意味着没有形成集合S，所以RNN和MLP是无法进行Instance Normalization操作的，这个很好理解。
6. Group Normalization 如何做？
	
	从Layer Normalization和Instance Normalization可以看出，这是两种极端情况，Layer Normalization是将同层所有神经元作为统计范围，而Instance Normalization则是CNN中将同一卷积层中每个卷积核对应的输出通道单独作为自己的统计范围。那么，有没有介于两者之间的统计范围呢？通道分组是CNN常用的模型优化技巧，所以自然而然会想到对CNN中某一层卷积层的输出或者输入通道进行分组，在分组范围内进行统计。这就是Group Normalization的核心思想，是Facebook何凯明研究组2017年提出的改进模型。
	
7. BN为何有效？

	原始的BN论文给出的解释是BN可以解决神经网络训练过程中的ICS（Internal Covariate Shift）问题，所谓ICS问题，指的是由于深度网络由很多隐层构成，在训练过程中由于底层网络参数不断变化，导致上层隐层神经元激活值的分布逐渐发生很大的变化和偏移，而这非常不利于有效稳定地训练神经网络。**不过后来经过学者研究发现BN和ICS并没有什么关系**
	
	BN与损失曲面有关，在深度网络叠加大量非线性函数方式来解决非凸复杂问题时，损失曲面形态异常复杂，大量空间坑坑洼洼相当不平整，也有很多空间是由平坦的大量充满鞍点的曲面构成，训练过程就是利用SGD在这个复杂平面上一步一步游走，期望找到全局最小值，也就是曲面里最深的那个坑。所以在SGD寻优时，在如此复杂曲面上寻找全局最小值而不是落入局部最小值或者被困在鞍点动弹不得，可想而知难度有多高。
	
	研究表明，BN真正的用处在于：通过上文所述的Normalization操作，使得网络参数重整（Reparametrize），它对于非线性非凸问题复杂的损失曲面有很好的平滑作用，参数重整后的损失曲面比未重整前的参数损失曲面平滑许多(平滑程度用L-Lipschitz函数来评估)。
	
8. **总结**

	所有Normalization模型都采取了类似的步骤和过程，将神经元的激活值重整为均值为0方差为1的新数值，最大的不同在于计算统计量的神经元集合S的划分方法上。BN采用了同一个神经元，但是来自于Mini-Batch中不同训练实例导致的不同激活作为统计范围。而为了克服Mini-Batch带来的弊端，后续改进方法抛弃了Mini-Batch的思路，只用当前训练实例引发的激活来划分集合S的统计范围，概括而言，LayerNorm采用同隐层的所有神经元；InstanceNorm采用CNN中卷积层的单个通道作为统计范围，而GroupNorm则折衷两者，采用卷积层的通道分组，在划分为同一个分组的通道内来作为通道范围。
	
	**使用场景:** 对于RNN的神经网络结构来说，目前只有LayerNorm是相对有效的；如果是GAN等图片生成或图片内容改写类型的任务，可以优先尝试InstanceNorm；如果使用场景约束BatchSize必须设置很小，无疑此时考虑使用GroupNorm；而其它任务情形应该优先考虑使用BatchNorm。
